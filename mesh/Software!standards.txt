Evaluation of IV compatibility microcomputer software programs.	Four microcomputer software programs that contain IV compatibility information were evaluated using specific criteria for these programs. This article compares scope of coverage, quality of clinical documentation, frequency updates, and clinical performance for these programs. There are no excellent IV compatibility programs. Of the programs tested, Micromedex performed the best. Therapeutic Software is not considered a comprehensive IV compatibility program. IV-Check PC and Medicom Micro performed equally well.
The second time around: reaching for the right package.	There are several key questions to ask when replacing a software package. Taking the time to get the answers will save a lot of time, money and grief.
Who's in charge here? You or your computer?!	To pinpoint problems, look at hardware, software, support training and the user. The problem may reside in any one, or several, of these areas.
Software upgrade can end with hard landing.	A hospital's plan to switch over to the latest version of software for its information systems often can result in a nasty surprise--the facility's hardware isn't up to the task, and the solution will cost a bundle, often hundreds of thousands of dollars. Providing your vendor with thorough and precise data can help avoid such pitfalls.
The trials, tribulations of being a guinea pig.	Turning your hospital into a laboratory for developing new computer products is "not something for the foolish or faint-hearted," warned one information systems manager. Joint-development agreements can be risky business and cause many headaches for all parties, but the payback for hospitals can be a big price break or even a major competitive edge.
Medical EDI message specification and interchange formats.	Assessment of the current medical edi environment learns that "old" interchange formats are being used to represent information of highly complex and quickly evolving nature. Whenever the domain requirements are translated into acceptable messages according to these interchange format specifications, system developers use the latest technologies to implement the messages towards the distorted view imposed by the interchange format constraints. This situation, combined with the fact of coexistence of several interchange formats and the urgent need for more standardised message types, led to the specification of a new meta-syntax, supporting easy message definition and allowing the data-driven conversion between multiple interchange formats. As such, and supported by the development of interchange format independent message descriptions, it may become a tool to support the medical edi needs of the future.
A proposed standard for registration of coding schemes.	This paper describes a proposed European standard for the Registration of Coding Schemes used in electronic healthcare information exchange. The requirements for such a standard are outlined together with the manner in which the work to meet these requirements has been carried out. The paper includes a summary of the main points contained in the proposed standard and in the supporting documents delivered by the project team. The paper concludes with a report on the progress of this proposal towards acceptance as a standard and a brief comment on how it may influence other work in medical informatics standardisation.
A model for coding and multiaxial classification examplified.	A model of a coding and classification scheme is examplified in the domain of clinical laboratory sciences. It is for a multilingual thesaurus based on coded elementary concepts used to form coded composites or aggregates of preferred terms. These are for transmission over alphabet and language barriers.
Models for representation of terminologies and coding systems in medicine.	Unification of medical coding systems is a perceived need involving a long-term task. Standards will support convergence of present coding systems towards a coherent set of tools. The Project Team "Model for Representation of Semantics" (CEN/TC251/PT003-MOSE) modeled existing tools to obtain a vocabulary apt to describe coding systems and terminologies. Requirements on faithfulness and safety, usefulness and purposiveness, coherence and integration were worked out, and the MOSE's 10 key principles for Standards in Medical Semantics were established.
See you in court. Damage inflicted by the Y2K bug threatens to become a dream for litigators but a nightmare for healthcare executives.	Most hospitals and health systems perceive and treat the year-2000 faulty computer code as a technical problem, but it is a high-level management problem. If hospitals don't start preparing comprehensively, they will face a host of consequences. Experts say spillover will extend into business issues, finances and liability. But providers must choose their most important priorities and develop contingency plans.
Hang in there! Quick takes on Y2K.	Cliffhangers in movies are great fun. But not in real life. Think Y2K. If you haven't scripted a way to beat the millennium bug, there are workable solutions that can be created in six months. Here's how to begin production: Prioritize activities, expenditures and resources, and apply them to the most vital problems first.
In search of the perfect laboratory computer system.	Some personal views on the selection of an ideal laboratory computer system are presented in this paper. Topics discussed include a review of some of the essential requirements for the ideal system; a survey of some of the commercial turn-key laboratory computer systems that are available; typical future trends that should be noted; and lastly, some criteria that may be used in the evaluation and selection of the ultimate system.
Quality assurance guidelines for a biomedical information web system: the working experience of the BreakIT project.	The paper outlines the quality control issues that arise in the implementation of a Web site which delivers biomedical information. At the heart of this study is the need for a methodology to guarantee that the information contained in the site is viable and accurate from both an informatic and a content point of view. This methodology is currently adopted in the Web development of the BreakIT project sponsored by the EU INFO2000 Programme.
TEIS--a system for public health offices for assessing, presenting, evaluating and communicating data on the quality of drinking water]	Monitoring the quality of drinking water is a cardinal task of German Public Health Offices and of the relevant Ministries of Health of the German Federal states ("Länder"). Today this can be tackled on a large scale and economically only with computer assistance. A system has been developed in North Rhine Westphalia on behalf of the Ministry of Health, for data assessment and communication which is suitable for practical work and user friendly. It aims at supporting the Public Health Offices in their daily work and at improving and simplifying the monitoring of drinking water supply systems and of drinking water quality control.
A lay person versus a trained endoscopist: can the preop endoscopy simulator detect a difference?	The purpose of this study was to establish construct validation of a flexible sigmoidoscopy simulator by comparing training-level grouped subjects. These included clerical staff (n = 10), residents (n = 19), and experts (n = 5). Each participant performed 3 scopes. The ANOVA group-based results for trainer-measured variables are shown in Table 1. These results demonstrate that the flexible sigmoidoscopy simulator distinguished the trained from the untrained and the resident from the expert. Although there was no statistically significant differences between the senior residents and the experts, the expert commonly outperformed the residents. Establishing the transferability of simulator training to real life is next. If the transfer of skill can be established, it may give rise to a new skills training approach.
Real time PCR methodology for quantification of nucleic acids]	The polymerase chain reaction (PCR) has become an essential tool for molecular biologists and its introduction into nucleic acids detection systems has revolutionized the quantitative analysis of DNA and RNA. The technique has rapidly evolved over the last few years and the growing interest in quantitative applications of the PCR has favoured the development of real-time quantitative PCR. In this paper, we review, after presentation of the theorical aspects of PCR, the basic principles of real-time PCR with the introduction of the concept of threshold cycle. More precisely, we describe the novel assay formats that greatly simplify the protocols used for the detection of specific nucleic acids. We focus on the actual four technologies that enable sequence detection in a closed tube and that are SYBR Green I, TaqMan probes, Hybridization probes and Molecular Beacon probes. We then discuss the different quantification strategies in real time PCR and compare the competiting instruments on the market. The most important real-time PCR applications in clinical biology are also described.
Open Source software in medical informatics--why, how and what.	'Open Source' is a 20-40 year old approach to licensing and distributing software that has recently burst into public view. Against conventional wisdom this approach has been wildly successful in the general software market--probably because the openness lets programmers the world over obtain, critique, use, and build upon the source code without licensing fees. Linux, a UNIX-like operating system, is the best known success. But computer scientists at the University of California, Berkeley began the tradition of software sharing in the mid 1970s with BSD UNIX and distributed the major internet network protocols as source code without a fee. Medical informatics has its own history of Open Source distribution: Massachusetts General's COSTAR and the Veterans Administration's VISTA software have been distributed as source code at no cost for decades. Bioinformatics, our sister field, has embraced the Open Source movement and developed rich libraries of open-source software. Open Source has now gained a tiny foothold in health care (OSCAR GEHR, OpenEMed). Medical informatics researchers and funding agencies should support and nurture this movement. In a world where open-source modules were integrated into operational health care systems, informatics researchers would have real world niches into which they could engraft and test their software inventions. This could produce a burst of innovation that would help solve the many problems of the health care system. We at the Regenstrief Institute are doing our part by moving all of our development to the open-source model.
The Byrne Guide for Inclusionary Cultural Content.	This article describes the background for development of The Byrne Guide for Inclusionary Cultural Content, which may help nurse educators evaluate and create instructional materials that include diverse groups. Curricular implications of diversity, as well as six categories of instructional bias are included in the background of information that contributed to the development of this Guide.
The mammalian protein-protein interaction database and its viewing system that is linked to the main FANTOM2 viewer.	Here, we describe the development of a mammalian protein-protein interaction (PPI) database and of a PPI Viewer application to display protein interaction networks (http://fantom21.gsc.riken.go.jp/PPI/). In the database, we stored the mammalian PPIs identified through our PPI assays (internal PPIs), as well as those we extracted and processed (external PPIs) from publicly available data sources, the DIP and BIND databases and MEDLINE abstracts by using FACTS, a new functional inference and curation system. We integrated the internal and external PPIs into the PPI database, which is linked to the main FANTOM2 viewer. In addition, we incorporated into the PPI Viewer information regarding the luciferase reporter activity of internal PPIs and the data confidence of external PPIs; these data enable visualization and evaluation of the reliability of each interaction. Using the described system, we successfully identified several interactions of biological significance. Therefore, the PPI Viewer is a useful tool for exploring FANTOM2 clone-related protein interactions and their potential effects on signaling and cellular communication.
PipeAlign: A new toolkit for protein family analysis.	PipeAlign is a protein family analysis tool integrating a five step process ranging from the search for sequence homologues in protein and 3D structure databases to the definition of the hierarchical relationships within and between subfamilies. The complete, automatic pipeline takes a single sequence or a set of sequences as input and constructs a high-quality, validated MACS (multiple alignment of complete sequences) in which sequences are clustered into potential functional subgroups. For the more experienced user, the PipeAlign server also provides numerous options to run only a part of the analysis, with the possibility to modify the default parameters of each software module. For example, the user can choose to enter an existing multiple sequence alignment for refinement, validation and subsequent clustering of the sequences. The aim is to provide an interactive workbench for the validation, integration and presentation of a protein family, not only at the sequence level, but also at the structural and functional levels. PipeAlign is available at http://igbmc.u-strasbg.fr/PipeAlign/.
Optimization of computer software settings improves accuracy of pulsed-field gel electrophoresis macrorestriction fragment pattern analysis.	Computer-assisted analysis of pulsed-field gel electrophoresis (PFGE) libraries can facilitate comparisons of fragment patterns present on multiple gels. We evaluated the ability of the Advanced Analysis (version 4.01) and Database (version 1.12) modules of the Phoretix gel analysis software package (Nonlinear USA, Inc., Durham, N.C.) to accurately match DNA fragment patterns. Two gels containing 38 lanes of SmaI-digested Enterococcus faecalis OG1RF DNA were analyzed to assess the impact of (i) varying the lane position of the standards, (ii) using gel plugs made at different times, and (iii) normalizing the fragment patterns by using molecular weight (MW) algorithms versus retardation factor (R(f)) algorithms. Two sets of PFGE libraries (one containing SmaI restriction patterns from 62 Enterococcus faecium isolates and the other containing SmaI restriction patterns of 89 Staphylococcus aureus isolates) were analyzed to assess the impact of varying the matching tolerance algorithm (designated as the vector box setting [VBS]) in the Phoretix software. Varying the lane position of standards on a gel and using gel plugs made on different days resulted in different VBSs, although it was not possible to judge whether those differences were statistically significant. Normalization of E. faecalis OG1RF fragment patterns by R(f) and MW methodology yielded no statistically significant differences in variability between the same fragment on different lanes. Suboptimal VBSs decreased the specificity with which related isolates were grouped together in dendrograms. The optimal VBS for analysis of PFGE fragment patterns from E. faecalis isolates differed from that for S. aureus isolates and sometimes was not that recommended by the manufacturer. Thus, computer-assisted analysis of PFGE patterns seemed to compensate for the intra- and intergel variation evaluated in the present study, and optimizing the software for the species to be tested was a critical preliminary step before further PFGE library analysis.
"Touch" vs. "tech": valuing nursing-specific PDA software.	Maximize your time by using a personal digital assistant (PDA) efficiently and effectively, featuring new nursing-specific software.
The latest MML (Medical Markup Language) version 2.3--XML-based standard for medical data exchange/storage.	As a set of standards, Medical Markup Language (MML) has been developed over the last 8 years to allow the exchange of medical data between different medical information providers MML version 2.21 was characterized by XML as metalanguage and was announced in 1999, at which time full-scale implementation tests were carried out; subsequently, various information and functional inadequacies were discovered in this version. MML was therefore updated to version 2.3 in 2001. At present, MML contains 12 MML modules including the new referral, test result, and report modules. In version 2.3, the group ID element was added; the access right definition and health insurance module were amended.
Herding cats: the challenges of EMR vendor selection.	The selection of an enterprise-wide electronic medical record (EMR) by a medical center is a major undertaking that will define its future clinical processes for many years. The parameters that drive the selection include the clinical requirements, the financial needs of the medical center, the geographic setting, the need for outreach into the community, and an analysis of the existing and predicted flow of information and work within the clinical systems.
Repeatability and reproducibility of ribotyping and its computer interpretation.	Many molecular typing methods are difficult to interpret because their repeatability (within-laboratory variance) and reproducibility (between-laboratory variance) have not been thoroughly studied. In the present work, ribotyping of coryneform bacteria was the basis of a study involving within-gel and between-gel repeatability and between-laboratory reproducibility (two laboratories involved). The effect of different technical protocols, different algorithms, and different software for fragment size determination was studied. Analysis of variance (ANOVA) showed, within a laboratory, that there was no significant added variance between gels. However, between-laboratory variance was significantly higher than within-laboratory variance. This may be due to the use of different protocols. An experimental function was calculated to transform the data and make them compatible (i.e., erase the between-laboratory variance). The use of different interpolation algorithms (spline, Schaffer and Sederoff) was a significant source of variation in one laboratory only. The use of either Taxotron (Institut Pasteur) or GelCompar (Applied Maths) was not a significant source of added variation when the same algorithm (spline) was used. However, the use of Bio-Gene (Vilber Lourmat) dramatically increased the error (within laboratory, within gel) in one laboratory, while decreasing the error in the other laboratory; this might be due to automatic normalization attempts. These results were taken into account for building a database and performing automatic pattern identification using Taxotron. Conversion of the data considerably improved the identification of patterns irrespective of the laboratory in which the data were obtained.
Using a computer to perform statistical analysis.	SPSS, and other statistical packages, make it easy to perform complex statistical analysis, but even simple analysis such as tables and graphical output are much simpler using such a package. Use of such a package does, unfortunately, also allow you to perform meaningless statistics and incorrect statistical tests, and give misleading or wrong interpretations. You will still need to understand some statistics, but you will not need to be able to compute the results yourself. A statistics package allows you to concentrate on the appropriateness of a test and interpretation of the results. It does not do the whole job for you.
Informatics approaches to functional MRI odor mapping of the rodent olfactory bulb: OdorMapBuilder and OdorMapDB.	The present study applies informatics tools to aid and extend fMRI analysis of the coding mechanism of neural signals in the rodent olfactory system. Odor stimulation evokes unique spatial patterns of activity in the glomerular layer of the mammalian olfactory bulb (OB). An open-source software program, OdorMap-Builder, has been developed to process the high resolution anatomical and functional MRI images of the OB and to generate single two-dimensional flat maps, called odor maps, that describe the spatial activity patterns in the entire glomerular layer. Odor maps help identify the spatial activity patterns from the tremendous amount of fMRI data and they serve as ideal representation of space coding for the olfactory signals in the OB in response to a given odor stimulation. Based on the fMRI technology, OdorMapBuilder provides comparable odor maps on the intra-subject basis, a significant step towards the detailed analyses of the effects of odor types and/or concentrations. In addition, a new database, OdorMapDB, is developed to provide a repository for the generated odor maps. Web interfaces to the database are provided for the data entry, modification and retrieval. OdorMapDB is based on the EAV/CR (entity-attribute-value with classes and relationships) architecture and it is integrated with two other SenseLab olfactory databases: the olfactory receptor and odor databases. Both OdorMapBuilder and OdorMapDB should serve as useful tools and resources for the field and help facilitate experimental research in understanding the olfactory system and the mechanism for smell perception.
The RUMBA software: tools for neuroimaging data analysis.	The enormous scale and complexity of data sets in functional neuroimaging makes it crucial to have well-designed and flexible software for image processing, modeling, and statistical analysis. At present, researchers must choose between general purpose scientific computing environments (e.g., Splus and Matlab), and specialized human brain mapping packages that implement particular analysis strategies (e.g., AFNI, SPM, VoxBo, FSL or FIASCO). For the vast majority of users in Human Brain Mapping and Cognitive Neuroscience, general purpose computing environments provide an insufficient framework for a complex data-analysis regime. On the other hand, the operational particulars of more specialized neuroimaging analysis packages are difficult or impossible to modify and provide little transparency or flexibility to the user for approaches other than massively multiple comparisons based on inferential statistics derived from linear models. In order to address these problems, we have developed open-source software that allows a wide array of data analysis procedures. The RUMBA software includes programming tools that simplify the development of novel methods, and accommodates data in several standard image formats. A scripting interface, along with programming libraries, defines a number of useful analytic procedures, and provides an interface to data analysis procedures. The software also supports a graphical functional programming environment for implementing data analysis streams based on modular functional components. With these features, the RUMBA software provides researchers programmability, reusability, modular analysis tools, novel data analysis streams, and an analysis environment in which multiple approaches can be contrasted and compared. The RUMBA software retains the flexibility of general scientific computing environments while adding a framework in which both experts and novices can develop and adapt neuroimaging-specific analyses.
A web-based federated neuroinformatics model for surgical planning and clinical research applications in epilepsy.	There is an increasing need to efficiently share diverse clinical and image data among different clinics, labs, and departments of a medical center enterprise to facilitate better quality care and more effective clinical research. In this paper, we describe a web-based, federated information model as a viable technical solution with applications in medical refractory epilepsy and other neurological disorders. We describe four such online applications developed in a federated system prototype: surgical planning, image analysis, statistical data analysis, and dynamic extraction, transforming, and loading (ETL) of data from a heterogeneous collection of data sources into an epilepsy multimedia data warehouse (EMDW). The federated information system adopts a three-tiered architecture, consisting of a user-interface layer, an application logic layer, and a data service layer. We implemented two complementary federated information technologies, i.e., XML (eXtensible Markup Language) and CORBA (Common Object Request Broker Architecture), in the prototype to enable multimedia data exchange and brain images transmission. The preliminary results show that the federated prototype system provides a uniform interface, heterogeneous information integration and efficient data sharing for users in our institution who are concerned with the care of patients with epilepsy and who pursue research in this area.
pdb-care (PDB carbohydrate residue check): a program to support annotation of complex carbohydrate structures in PDB files.	Carbohydrates are involved in a variety of fundamental biological processes and pathological situations. They therefore have a large pharmaceutical and diagnostic potential. Knowledge of the 3D structure of glycans is a prerequisite for a complete understanding of their biological functions. The largest source of biomolecular 3D structures is the Protein Data Bank. However, about 30% of all 1663 PDB entries (version September 2003) containing carbohydrates comprise errors in glycan description. Unfortunately, no software is currently available which aligns the 3D information with the reported assignments. It is the aim of this work to fill this gap.
Software reliability prediction using recurrent neural network with Bayesian regularization.	A recurrent neural network modeling approach for software reliability prediction with respect to cumulative failure time is proposed. Our proposed network structure has the capability of learning and recognizing the inherent internal temporal property of cumulative failure time sequence. Further, by adding a penalty term of sum of network connection weights, Bayesian regularization is applied to our network training scheme to improve the generalization capability and lower the susceptibility of overfitting. The performance of our proposed approach has been tested using four real-time control and flight dynamic application data sets. Numerical results show that our proposed approach is robust across different software projects, and has a better performance with respect to both goodness-of-fit and next-step-predictability compared to existing neural network models for failure time prediction.
Random allocation software for parallel group randomized trials.	Typically, randomization software should allow users to exert control over the different aspects of randomization including block design, provision of unique identifiers and control over the format and type of program output. While some of these characteristics have been addressed by available software, none of them have all of these capabilities integrated into one package. The main objective of the Random Allocation Software project was to enhance the user's control over different aspects of randomization in parallel group trials, including output type and format, structure and ordering of generated unique identifiers and enabling users to specify group names for more than two groups.
Using software to direct test construction and improve test performance.	Over the past 5 years, a test analysis software program was developed and is being successfully used to modify faculty-constructed nursing tests administered to small classes. The interplay of instruction, students, and test items were variables considered while developing a practical, efficient program to produce test performance printouts designed to be used with little need for statistical interpretation. This test performance profile software is now available as freeware for individual nursing faculty.
Gene and alternative splicing annotation with AIR.	Designing effective and accurate tools for identifying the functional and structural elements in a genome remains at the frontier of genome annotation owing to incompleteness and inaccuracy of the data, limitations in the computational models, and shifting paradigms in genomics, such as alternative splicing. We present a methodology for the automated annotation of genes and their alternatively spliced mRNA transcripts based on existing cDNA and protein sequence evidence from the same species or projected from a related species using syntenic mapping information. At the core of the method is the splice graph, a compact representation of a gene, its exons, introns, and alternatively spliced isoforms. The putative transcripts are enumerated from the graph and assigned confidence scores based on the strength of sequence evidence, and a subset of the high-scoring candidates are selected and promoted into the annotation. The method is highly selective, eliminating the unlikely candidates while retaining 98% of the high-quality mRNA evidence in well-formed transcripts, and produces annotation that is measurably more accurate than some evidence-based gene sets. The process is fast, accurate, and fully automated, and combines the traditionally distinct gene annotation and alternative splicing detection processes in a comprehensive and systematic way, thus considerably aiding in the ensuing manual curation efforts.
Comparison of programmable shunt valves vs standard valves for communicating hydrocephalus of adults: a retrospective analysis of 407 patients.	The aim of the present study was to clarify whether programmable shunt valves are advantageous in the treatment of adults with communicating hydrocephalus with respect to valve-related shunt complications and surgical shunt revisions, in comparison with standard valves.
Quantitative comparison of algorithms for inter-subject registration of 3D volumetric brain MRI scans.	The objective of inter-subject registration of three-dimensional volumetric brain scans is to reduce the anatomical variability between the images scanned from different individuals. This is a necessary step in many different applications such as voxelwise group analysis of imaging data obtained from different individuals. In this paper, the ability of three different image registration algorithms in reducing inter-subject anatomical variability is quantitatively compared using a set of common high-resolution volumetric magnetic resonance imaging scans from 17 subjects. The algorithms are from the automatic image registration (AIR; version 5), the statistical parametric mapping (SPM99), and the automatic registration toolbox (ART) packages. The latter includes the implementation of a non-linear image registration algorithm, details of which are presented in this paper. The accuracy of registration is quantified in terms of two independent measures: (1) post-registration spatial dispersion of sets of homologous landmarks manually identified on images before or after registration; and (2) voxelwise image standard deviation maps computed within the set of images registered by each algorithm. Both measures showed that the ART algorithm is clearly superior to both AIR and SPM99 in reducing inter-subject anatomical variability. The spatial dispersion measure was found to be more sensitive when the landmarks were placed after image registration. The standard deviation measure was found sensitive to intensity normalization or the method of image interpolation.
Audiophile hardware in vision science; the soundcard as a digital to analog converter.	The design objective was to develop an inexpensive digital to analog (D/A) converter for use in vision science. Soundcards are hardware units that can be integral or can be added to a computer to add sound capability. A soundcard contains D/A converters designed to work in the audio frequency range, typically 20-20,000 Hz. Soundcard outputs are high-pass filtered and thus do not convey sub-audio frequency or dc information. It is possible to circumvent this design feature by programming the desired output waveform as an amplitude modulation of a high frequency carrier, and then demodulating the soundcard output. The circuit, using a 20 kHz carrier, provides precise D/A conversion for the frequency range relevant for vision experiments, dc to 100 Hz, using inexpensive readily available components. The specific application was for 8 channels of D/A conversion using a Macintosh computer running under OS X. The software needed to program stimuli was created using CoreAudio, a library for programming sounds in OS X. Using soundcards on other platforms would not be a problem, as long as there exists a low level library that would enable the wave table to be filled.
Emotion language in primary care encounters: reliability and validity of an emotion word count coding system.	To develop a reliable and valid computer coded measure to assess emotional expression from transcripts of physician-patient interactions.
Using Lotus software to calculate faculty merit increases.	Evaluation of college and university faculty is an annual occurrence in most institutions. The evaluation process is time consuming and may not result in the desired outcome: a discriminating, objective, and comprehensive evaluation of faculty performance. Merit increases are based on the results of faculty performance evaluations. The lack of precision and the complexity of the process supports the adoption of a computer-based approach to calculate faculty merit increases. Lotus is a powerful and sophisticated software program that supports the organization, analysis, and reporting of large amounts of information. This article describes the process for developing a computer spread sheet system for calculating faculty merit increases.
Breaking the hype cycle: using the computer effectively with learners with intellectual disabilities.	There has been huge growth in the use of information technology (IT) in classrooms for learners of all ages. It has been suggested that computers in the classroom encourage independent and self-paced learning, provide immediate feedback and improve self-motivation and self-confidence. Concurrently there is increasing interest related to the role of technology in educational programs for individuals with intellectual disabilities. However, although many claims are made about the benefits of computers and software packages there is limited evidence based information to support these claims. Researchers are now starting to look at the specific instructional design features that are hypothesised to facilitate education outcomes rather than the over-emphasis on graphics and sounds. Research undertaken as part of a post-school program (Latch-On: Literacy and Technology - Hands On) at the University of Queensland investigated the use of computers by young adults with intellectual disabilities. The aims of the research reported in this paper were to address the challenges identified in the 'hype' surrounding different pieces of educational software and to develop a means of systematically analysing software for use in teaching programs.
Proof-of-concept design and development of an EN13606-based electronic health care record service.	The authors present an Electronic Healthcare Record (EHR) server, designed and developed as a proof of concept of the revised prEN13606:2005 European standard concerning EHR communications.
A systematic review of computer-based softwares for educating patients with coronary heart disease.	To evaluate the use of computer-based softwares for educating patients with coronary heart disease.
Development and quality assurance of computer-based assessment batteries.	The purpose of this article is to outline critical elements in the development and quality assurance (QA) assessment of a computer-based assessment battery (CAB). The first section of the article provides an overview of the life cycle of a representative CAB, typical evolutionary stages, and many of the essential considerations for designing and developing a CAB. The second section of the article presents a model for conducting a quality assurance assessment of a CAB. A general narrative of several steps in the QA process is supported by a table of recommended QA assessment elements. Although this QA process model may not be definitive for all cases, it provides a general framework within which a systematic assessment of any CAB can be conducted.
Development and validation of a computer program to design and calculate ROPS.	In Spain, there are more than 250,000 tractors built before 1980, when it became mandatory for all new tractors to be equipped with a rollover protective structure (ROPS). A similar situation is found in the European Union, but the situation is worse in the U.S. and in developing countries. Directive 2003/37/EEC establishes that tractors over 800 kg weight can be homologated by using the OECD standard code for the official testing of protective structures on agricultural and forestry tractors (static test), called Code 4. A ROPS attachable to the rear axle of different tractor models has been designed, and a computer program for the calculation of the ROPS design has been developed. The program, named ESTREMA, is available at: www.cfnavarra.es/insl. Using this program, it has been possible to design a ROPS for the Massey Ferguson model 178 tractor, one of the most common tractor models without a ROPS in Spain. After the tractor was equipped with the designed ROPS, it was tested at the Spanish Authorized Station for testing ROPS and passed the homologation test (OECD Code 4), the main results being a maximum distortion of 21.3 cm when the absorbed energy was 5437 N and the maximum force applied was 34 kN during loading from the side. The ROPS was improved, redesigned, and remounted on the tractor, the tractor was tested in a real overturn, and no part of the structure intruded on the driver's clearance zone during the test. In conclusion, the ESTREMA program worked correctly, and the designed ROPS was able to pass the authorized test and provide adequate protection to the operator during a real overturn.
Patient care automation: the future is now. Part 5. The role of technology.	A number of technical issues have been considered in this article; such issues need to be considered in a patient care systems procurement after the philosophic base of the system has been established.
Physician, heal thy software.	This article begins with the view that computers are not essential for medical audit. There is a lot of pressure from manufacturers and enthusiasts to purchase computers as a prerequisite to undertaking medical audit. The authors challenge this view and provide a useful checklist of questions to ask before deciding to buy a computer and discuss how to choose the right system for your needs, once the decision has been made.
Programming in the small.	Academic medical centers, in general, and radiation oncology research, in particular, rely heavily on custom software tools and applications. The code development is typically the responsibility of a single individual or at most a small team. Often these individuals are not professional programmers but physicists, students, and physicians. While they possess domain expertise and algorithm knowledge, they often are not fully aware of general "safe coding" practices--nor do they need the full complexity familiar in large commercial software projects to succeed. Rather, some simple guidelines we refer to as "programming in the small" can be used.
An educational documentation system for a hospital nursing education department.	This article explains the development of a computerized, educational record-keeping system. Steps in the decision-making process, as well as options to be considered, are detailed. This information will help nursing staff development educators implement or change their own nursing education documentation system.
Using change dynamics to implement a new software system in the OR.	Even though goals are essential, maintenance of rapport and communication must not be overlooked when the leader focuses on the overall plan. When the leader shows understanding toward those living with the day-to-day challenges of change, the staff and leader are brought together as a team. The effective leader also must learn to distinguish between barriers that must be confronted and those that should be avoided to lessen potential for permanent damage to relationships. Change managed effectively provides opportunities for growth as a team as well as integration of new practices.
Influence of nodule detection software on radiologists' confidence in identifying pulmonary nodules with computed tomography.	With advances in technology, detection of small pulmonary nodules is increasing. Nodule detection software (NDS) has been developed to assist radiologists with pulmonary nodule diagnosis. Although it may increase sensitivity for small nodules, often there is an accompanying increase in false-positive findings. We designed a study to examine the extent to which computed tomography (CT) NDS influences the confidence of radiologists in identifying small pulmonary nodules.
The Canadian Problem Gambling Index: an evaluation of the scale and its accompanying profiler software in a clinical setting.	Across two studies we assessed the clinical utility of the Canadian Problem Gambling Index (CPGI). In Study 1, the scored items on the CPGI significantly correlated with those of the South Oaks Gambling Screen (SOGS), yet their shared variance was low. Importantly, clinician evaluation of the client's level of pathology was more strongly associated with that revealed by the CPGI than the SOGS. In terms of utility, clinicians found the non-scored items on the CPGI more useful in treatment than those included with the SOGS. In Study 2, the effectiveness of the CPGI profiler (CPGI-P) software, which graphically depicts problematic gambling-relevant attitudes and behaviours, was assessed. Although clients had difficulties using the CPGI-P interface, they overwhelmingly indicated that the output prompted action to address their gambling. The clinicians were less enthusiastic as they felt the output did not help clients truly understand their gambling problems. Such sentiments were reiterated by the clinicians at a 6 months follow-up. The use of the SOGS and possible adoption of the CPGI (as well as the CPGI-P) in a clinical setting are discussed.
Technical note on the validation of a semi-automated image analysis software application for estrogen and progesterone receptor detection in breast cancer.	The immunohistochemical detection of estrogen (ER) and progesterone (PR) receptors in breast cancer is routinely used for prognostic and predictive testing. Whole slide digitalization supported by dedicated software tools allows quantization of the image objects (e.g. cell membrane, nuclei) and an unbiased analysis of immunostaining results. Validation studies of image analysis applications for the detection of ER and PR in breast cancer specimens provided strong concordance between the pathologist's manual assessment of slides and scoring performed using different software applications.
Accuracy of CNV Detection from GWAS Data.	Several computer programs are available for detecting copy number variants (CNVs) using genome-wide SNP arrays. We evaluated the performance of four CNV detection software suites--Birdsuite, Partek, HelixTree, and PennCNV-Affy--in the identification of both rare and common CNVs. Each program's performance was assessed in two ways. The first was its recovery rate, i.e., its ability to call 893 CNVs previously identified in eight HapMap samples by paired-end sequencing of whole-genome fosmid clones, and 51,440 CNVs identified by array Comparative Genome Hybridization (aCGH) followed by validation procedures, in 90 HapMap CEU samples. The second evaluation was program performance calling rare and common CNVs in the Bipolar Genome Study (BiGS) data set (1001 bipolar cases and 1033 controls, all of European ancestry) as measured by the Affymetrix SNP 6.0 array. Accuracy in calling rare CNVs was assessed by positive predictive value, based on the proportion of rare CNVs validated by quantitative real-time PCR (qPCR), while accuracy in calling common CNVs was assessed by false positive/false negative rates based on qPCR validation results from a subset of common CNVs. Birdsuite recovered the highest percentages of known HapMap CNVs containing >20 markers in two reference CNV datasets. The recovery rate increased with decreased CNV frequency. In the tested rare CNV data, Birdsuite and Partek had higher positive predictive values than the other software suites. In a test of three common CNVs in the BiGS dataset, Birdsuite's call was 98.8% consistent with qPCR quantification in one CNV region, but the other two regions showed an unacceptable degree of accuracy. We found relatively poor consistency between the two "gold standards," the sequence data of Kidd et al., and aCGH data of Conrad et al. Algorithms for calling CNVs especially common ones need substantial improvement, and a "gold standard" for detection of CNVs remains to be established.
4D in in vivo 2-photon laser scanning fluorescence microscopy with sample motion in 6 degrees of freedom.	2-Photon laser scanning microscopy (TPLSM) is often used for chronic in vivo studies. Small deviations in the sample orientation, however, make comparison of three-dimensional image stacks taken at different time-points challenging. When analysing changes of three-dimensional structures over time (4D imaging) this fundamental problem is one of the main limitations when complex structures are studied repetitively. We used an upright two-photon microscope complemented with a software-controlled stage-rotation instead of a conventional stage for chronic in vivo imaging in the brain of transgenic mouse models of Alzheimer's disease. Before every session an optimal imaging condition was successfully created by aligning the surface of the cranial window perfectly perpendicular to the laser beam. Deviations in the sample orientation between consecutive imaging sessions could be eliminated which improves conditions for chronic in vivo studies.
Comparative view of in silico DNA sequencing analysis tools.	DNA sequencing is an important tool for discovery of genetic variants. The task of detecting single-nucleotide variants is complicated by noise and sequencing artifacts in sequencing data. Several in silico tools have been developed to assist this process. These tools interpret the raw chromatogram data and perform a specialized base-calling and quality-control assessment procedure to identify variants. The approach used to identify variants differs between the tools, with some specific to SNPs and other for Indels. The choice of a tool is guided by the design of the sequencing project and the nature of the variant to be discovered. In this chapter, these tools are compared to facilitate the choice of a tool used for variant discovery.
Screening for glaucoma with Moorfields regression analysis and glaucoma probability score in confocal scanning laser ophthalmoscopy.	To compare the validity of Moorfields regression analysis (MRA) and glaucoma probability score (GPS) of the confocal scanning laser ophthalmoscopy (Heidelberg retina tomograph 3; HRT3) in detecting glaucomatous optic nerve damage in a screening population.
Is there a valid app for that? Validity of a free pedometer iPhone application.	This study examined the validity of a selected free pedometer application (iPedometer; IP) for the iPhone that could be used to assess physical activity.
Intelligent sensors security.	The paper is focused on the security issues of sensors provided with processors and software and used for high-risk applications. Common IT related threats may cause serious consequences for sensor system users. To improve their robustness, sensor systems should be developed in a restricted way that would provide them with assurance. One assurance creation methodology is Common Criteria (ISO/IEC 15408) used for IT products and systems. The paper begins with a primer on the Common Criteria, and then a general security model of the intelligent sensor as an IT product is discussed. The paper presents how the security problem of the intelligent sensor is defined and solved. The contribution of the paper is to provide Common Criteria (CC) related security design patterns and to improve the effectiveness of the sensor development process.
Air travel and vector-borne disease movement.	Recent decades have seen substantial expansions in the global air travel network and rapid increases in traffic volumes. The effects of this are well studied in terms of the spread of directly transmitted infections, but the role of air travel in the movement of vector-borne diseases is less well understood. Increasingly however, wider reaching surveillance for vector-borne diseases and our improving abilities to map the distributions of vectors and the diseases they carry, are providing opportunities to better our understanding of the impact of increasing air travel. Here we examine global trends in the continued expansion of air transport and its impact upon epidemiology. Novel malaria and chikungunya examples are presented, detailing how geospatial data in combination with information on air traffic can be used to predict the risks of vector-borne disease importation and establishment. Finally, we describe the development of an online tool, the Vector-Borne Disease Airline Importation Risk (VBD-Air) tool, which brings together spatial data on air traffic and vector-borne disease distributions to quantify the seasonally changing risks for importation to non-endemic regions. Such a framework provides the first steps towards an ultimate goal of adaptive management based on near real time flight data and vector-borne disease surveillance.
Computer programming: quality and safety for neonatal parenteral nutrition orders.	Computerized software programs reduce errors and increase consistency when ordering parenteral nutrition (PN). The purpose of this study was to evaluate the effectiveness of our computerized neonatal PN calculator ordering program in reducing errors and optimizing nutrient intake.
Structured representation for core elements of common clinical decision support interventions to facilitate knowledge sharing.	At present, there are no widely accepted, standard approaches for representing computer-based clinical decision support (CDS) intervention types and their structural components. This study aimed to identify key requirements for the representation of five widely utilized CDS intervention types: alerts and reminders, order sets, infobuttons, documentation templates/forms, and relevant data presentation. An XML schema was proposed for representing these interventions and their core structural elements (e.g., general metadata, applicable clinical scenarios, CDS inputs, CDS outputs, and CDS logic) in a shareable manner. The schema was validated by building CDS artifacts for 22 different interventions, targeted toward guidelines and clinical conditions called for in the 2011 Meaningful Use criteria. Custom style sheets were developed to render the XML files in human-readable form. The CDS knowledge artifacts were shared via a public web portal. Our experience also identifies gaps in existing standards and informs future development of standards for CDS knowledge representation and sharing. 
Design of the asthma treat smart system in a pediatric institution.	Asthma is one of the most common chronic pediatric conditions. Providing evidence-based, guideline-appropriate care for asthma is complex. A computerized system may help providers with guideline compliance. The AsthmaTreatSmart application is a stand-alone web-based system developed in Pulmonary medicine with a multidisciplinary team based on national asthma guidelines. The application collects history and symptom information to assign a severity categorization. Medication reconciliation of asthma medications is performed with input from the patient. Severity and medication recommendation are provided for the provider. A personalized action plan and medication summary including medications, etc. is created. The system is successfully used in 6 outpatient clinics and current work is focused on integration with the electronic health record. 
Using ISO 25040 standard for evaluating electronic health record systems.	Quality of electronic health record systems (EHR-S) is one of the key points in the discussion about the safe use of this kind of system. It stimulates creation of technical standards and certifications in order to establish the minimum requirements expected for these systems. [1] In other side, EHR-S suppliers need to invest in evaluation of their products to provide systems according to these requirements. This work presents a proposal of use ISO 25040 standard, which focuses on the evaluation of software products, for define a model of evaluation of EHR-S in relation to Brazilian Certification for Electronic Health Record Systems - SBIS-CFM Certification. Proposal instantiates the process described in ISO 25040 standard using the set of requirements that is scope of the Brazilian certification. As first results, this research has produced an evaluation model and a scale for classify an EHR-S about its compliance level in relation to certification. This work in progress is part for the acquisition of the degree of master in Computer Science at the Federal University of Pernambuco. 
Application of the AMPLE cluster-and-truncate approach to NMR structures for molecular replacement.	AMPLE is a program developed for clustering and truncating ab initio protein structure predictions into search models for molecular replacement. Here, it is shown that its core cluster-and-truncate methods also work well for processing NMR ensembles into search models. Rosetta remodelling helps to extend success to NMR structures bearing low sequence identity or high structural divergence from the target protein. Potential future routes to improved performance are considered and practical, general guidelines on using AMPLE are provided. 
A cloud-based X73 ubiquitous mobile healthcare system: design and implementation.	Based on the user-centric paradigm for next generation networks, this paper describes a ubiquitous mobile healthcare (uHealth) system based on the ISO/IEEE 11073 personal health data (PHD) standards (X73) and cloud computing techniques. A number of design issues associated with the system implementation are outlined. The system includes a middleware on the user side, providing a plug-and-play environment for heterogeneous wireless sensors and mobile terminals utilizing different communication protocols and a distributed "big data" processing subsystem in the cloud. The design and implementation of this system are envisaged as an efficient solution for the next generation of uHealth systems. 
An approach for the evaluation of software engineering environments in medicine.	This article examines several criteria for the evaluation of software engineering environments (SEE) in medicine. The study is restricted to the evaluation of the SEE itself, not of its by-products which are the medical applications developed with the SEE. Basic principles for an evaluation methodology are presented. They consist in determining the evaluation objectives and judging a SEE according to criteria which are grouped into three broad categories--functional, generic and environmental. Each category reflects a particular domain of evaluation of the SEE. Methods of measurement and questions highlighting these specific areas are mentioned. Criteria are extracted from the list of objectives that follows the HELIOS European AIM project of the Commission of the European Communities. Special emphasis is drawn on the criteria for which the medical specificity and usefulness of a SEE can be approach. For this purpose a method of measurement of such appropriateness is proposed.
Software requirements: definition and specification.	The software requirements specification is the single most important document in the software development process. It provides the basis for development as well as for validation. The SRS needs to include adequate definition of all requirements without specifying implementation or project management issues. The SRS should be completed early in the development process. However, it is very likely that changes will occur during the development life cycle. This is not an excuse for approving and releasing the current version of the SRS. When changes occur, the SRS must be revised. In any case, the concept is to deal with the current, approved version of the SRS. Ultimately, the SRS should include all the information needed to proceed into the design phase of software development.
Component-based development for supporting workflows in hospitals.	Changing requirements for health care information systems force the development of an open, modular architecture in which components can be integrated. This offers a flexible means for integrating different (heterogeneous) systems used by different users. Selecting the components to integrate, and determining the 'right way' to integrate them, necessitates a shift in focus towards the business process to be supported. The realization of such an open, modular architecture is a difficult task. It consists of breaking down existing systems in required components and integrating these and other components. Many authors on component-based development strategies focus attention on the technological issues of component integration (do we use CORBA, DCOM/OLE, or EDI?). This paper presents an approach for determining the required components, and the way they have to be integrated, based on an analysis of the business process to be supported, and the information systems currently used.
